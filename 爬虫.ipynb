{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 1 页爬取完毕！\n",
      "第 2 页爬取完毕！\n",
      "第 3 页爬取完毕！\n",
      "第 4 页爬取完毕！\n",
      "第 5 页爬取完毕！\n",
      "第 6 页爬取完毕！\n",
      "第 7 页爬取完毕！\n",
      "第 8 页爬取完毕！\n",
      "第 9 页爬取完毕！\n",
      "第 10 页爬取完毕！\n",
      "第 11 页爬取完毕！\n",
      "第 12 页爬取完毕！\n",
      "第 13 页爬取完毕！\n",
      "第 14 页爬取完毕！\n",
      "第 15 页爬取完毕！\n",
      "第 16 页爬取完毕！\n",
      "第 17 页爬取完毕！\n",
      "第 18 页爬取完毕！\n",
      "第 19 页爬取完毕！\n",
      "第 20 页爬取完毕！\n",
      "第 21 页爬取完毕！\n",
      "第 22 页爬取完毕！\n",
      "第 23 页爬取完毕！\n",
      "第 24 页爬取完毕！\n",
      "第 25 页爬取完毕！\n",
      "第 26 页爬取完毕！\n",
      "第 27 页爬取完毕！\n",
      "第 28 页爬取完毕！\n",
      "第 29 页爬取完毕！\n",
      "第 30 页爬取完毕！\n",
      "第 31 页爬取完毕！\n",
      "第 32 页爬取完毕！\n",
      "第 33 页爬取完毕！\n",
      "第 34 页爬取完毕！\n",
      "第 35 页爬取完毕！\n",
      "第 36 页爬取完毕！\n",
      "第 37 页爬取完毕！\n",
      "第 38 页爬取完毕！\n",
      "第 39 页爬取完毕！\n",
      "第 40 页爬取完毕！\n",
      "第 41 页爬取完毕！\n",
      "第 42 页爬取完毕！\n",
      "第 43 页爬取完毕！\n",
      "第 44 页爬取完毕！\n",
      "第 45 页爬取完毕！\n",
      "第 46 页爬取完毕！\n",
      "第 47 页爬取完毕！\n",
      "第 48 页爬取完毕！\n",
      "第 49 页爬取完毕！\n",
      "第 50 页爬取完毕！\n",
      "第 51 页爬取完毕！\n",
      "第 52 页爬取完毕！\n",
      "第 53 页爬取完毕！\n",
      "第 54 页爬取完毕！\n",
      "第 55 页爬取完毕！\n",
      "第 56 页爬取完毕！\n",
      "第 57 页爬取完毕！\n",
      "第 58 页爬取完毕！\n",
      "第 59 页爬取完毕！\n",
      "第 60 页爬取完毕！\n",
      "第 61 页爬取完毕！\n",
      "第 62 页爬取完毕！\n",
      "第 63 页爬取完毕！\n",
      "第 64 页爬取完毕！\n",
      "第 65 页爬取完毕！\n",
      "第 66 页爬取完毕！\n",
      "第 67 页爬取完毕！\n",
      "第 68 页爬取完毕！\n",
      "第 69 页爬取完毕！\n",
      "第 70 页爬取完毕！\n",
      "第 71 页爬取完毕！\n",
      "第 72 页爬取完毕！\n",
      "第 73 页爬取完毕！\n",
      "第 74 页爬取完毕！\n",
      "第 75 页爬取完毕！\n",
      "第 76 页爬取完毕！\n",
      "第 77 页爬取完毕！\n",
      "第 78 页爬取完毕！\n",
      "第 79 页爬取完毕！\n",
      "第 80 页爬取完毕！\n",
      "第 81 页爬取完毕！\n",
      "第 82 页爬取完毕！\n",
      "第 83 页爬取完毕！\n",
      "第 84 页爬取完毕！\n",
      "第 85 页爬取完毕！\n",
      "第 86 页爬取完毕！\n",
      "第 87 页爬取完毕！\n",
      "第 88 页爬取完毕！\n",
      "第 89 页爬取完毕！\n",
      "第 90 页爬取完毕！\n",
      "第 91 页爬取完毕！\n",
      "第 92 页爬取完毕！\n",
      "第 93 页爬取完毕！\n",
      "第 94 页爬取完毕！\n",
      "第 95 页爬取完毕！\n",
      "第 96 页爬取失败，进入下一页爬虫！\n",
      "第 97 页爬取完毕！\n",
      "第 98 页爬取完毕！\n",
      "第 99 页爬取完毕！\n",
      "第 100 页爬取完毕！\n",
      "第 101 页爬取完毕！\n",
      "第 102 页爬取完毕！\n",
      "第 103 页爬取完毕！\n",
      "第 104 页爬取完毕！\n",
      "第 105 页爬取完毕！\n",
      "第 106 页爬取完毕！\n",
      "第 107 页爬取完毕！\n",
      "第 108 页爬取完毕！\n",
      "第 109 页爬取完毕！\n",
      "第 110 页爬取完毕！\n",
      "第 111 页爬取完毕！\n",
      "第 112 页爬取完毕！\n",
      "第 113 页爬取完毕！\n",
      "第 114 页爬取完毕！\n",
      "第 115 页爬取完毕！\n",
      "第 116 页爬取完毕！\n",
      "第 117 页爬取完毕！\n",
      "第 118 页爬取完毕！\n",
      "第 119 页爬取完毕！\n",
      "第 120 页爬取完毕！\n",
      "第 121 页爬取完毕！\n",
      "第 122 页爬取完毕！\n",
      "第 123 页爬取完毕！\n",
      "第 124 页爬取完毕！\n",
      "第 125 页爬取完毕！\n",
      "第 126 页爬取完毕！\n",
      "第 127 页爬取完毕！\n",
      "第 128 页爬取完毕！\n",
      "第 129 页爬取完毕！\n",
      "第 130 页爬取完毕！\n",
      "第 131 页爬取完毕！\n",
      "第 132 页爬取完毕！\n",
      "第 133 页爬取完毕！\n",
      "第 134 页爬取完毕！\n",
      "第 135 页爬取完毕！\n",
      "第 136 页爬取完毕！\n",
      "第 137 页爬取完毕！\n",
      "第 138 页爬取完毕！\n",
      "第 139 页爬取完毕！\n",
      "第 140 页爬取完毕！\n",
      "第 141 页爬取完毕！\n",
      "第 142 页爬取完毕！\n",
      "第 143 页爬取完毕！\n",
      "第 144 页爬取完毕！\n",
      "第 145 页爬取完毕！\n",
      "第 146 页爬取完毕！\n",
      "第 147 页爬取完毕！\n",
      "第 148 页爬取完毕！\n",
      "第 149 页爬取完毕！\n",
      "第 150 页爬取完毕！\n",
      "第 151 页爬取完毕！\n",
      "第 152 页爬取完毕！\n",
      "第 153 页爬取完毕！\n",
      "第 154 页爬取完毕！\n",
      "第 155 页爬取完毕！\n",
      "第 156 页爬取完毕！\n",
      "第 157 页爬取完毕！\n",
      "第 158 页爬取完毕！\n",
      "第 159 页爬取完毕！\n",
      "第 160 页爬取完毕！\n",
      "第 161 页爬取完毕！\n",
      "第 162 页爬取完毕！\n",
      "第 163 页爬取完毕！\n",
      "第 164 页爬取完毕！\n",
      "第 165 页爬取完毕！\n",
      "第 166 页爬取完毕！\n",
      "第 167 页爬取完毕！\n",
      "第 168 页爬取完毕！\n",
      "第 169 页爬取完毕！\n",
      "第 170 页爬取完毕！\n",
      "第 171 页爬取完毕！\n",
      "第 172 页爬取失败，进入下一页爬虫！\n",
      "第 173 页爬取完毕！\n",
      "第 174 页爬取完毕！\n",
      "第 175 页爬取完毕！\n",
      "第 176 页爬取完毕！\n",
      "第 177 页爬取完毕！\n",
      "第 178 页爬取完毕！\n",
      "第 179 页爬取完毕！\n",
      "第 180 页爬取完毕！\n",
      "第 181 页爬取完毕！\n",
      "第 182 页爬取完毕！\n",
      "第 183 页爬取完毕！\n",
      "第 184 页爬取完毕！\n",
      "第 185 页爬取完毕！\n",
      "第 186 页爬取完毕！\n",
      "第 187 页爬取完毕！\n",
      "第 188 页爬取完毕！\n",
      "第 189 页爬取完毕！\n",
      "第 190 页爬取完毕！\n",
      "第 191 页爬取完毕！\n",
      "第 192 页爬取完毕！\n",
      "第 193 页爬取完毕！\n",
      "第 194 页爬取完毕！\n",
      "第 195 页爬取完毕！\n",
      "第 196 页爬取完毕！\n",
      "第 197 页爬取完毕！\n",
      "第 198 页爬取完毕！\n",
      "第 199 页爬取完毕！\n",
      "第 200 页爬取完毕！\n",
      "第 201 页爬取完毕！\n",
      "第 202 页爬取完毕！\n",
      "第 203 页爬取完毕！\n",
      "第 204 页爬取完毕！\n",
      "第 205 页爬取完毕！\n",
      "第 206 页爬取完毕！\n",
      "第 207 页爬取完毕！\n",
      "第 208 页爬取完毕！\n",
      "第 209 页爬取完毕！\n",
      "第 210 页爬取完毕！\n",
      "第 211 页爬取完毕！\n",
      "第 212 页爬取完毕！\n",
      "第 213 页爬取完毕！\n",
      "第 214 页爬取完毕！\n",
      "第 215 页爬取完毕！\n",
      "第 216 页爬取完毕！\n",
      "第 217 页爬取完毕！\n",
      "第 218 页爬取完毕！\n",
      "第 219 页爬取完毕！\n",
      "第 220 页爬取完毕！\n",
      "第 221 页爬取完毕！\n",
      "第 222 页爬取完毕！\n",
      "第 223 页爬取完毕！\n",
      "第 224 页爬取完毕！\n",
      "第 225 页爬取完毕！\n",
      "第 226 页爬取完毕！\n",
      "第 227 页爬取完毕！\n",
      "第 228 页爬取完毕！\n",
      "第 229 页爬取完毕！\n",
      "第 230 页爬取完毕！\n",
      "第 231 页爬取完毕！\n",
      "第 232 页爬取完毕！\n",
      "第 233 页爬取完毕！\n",
      "第 234 页爬取完毕！\n",
      "第 235 页爬取完毕！\n",
      "第 236 页爬取完毕！\n",
      "第 237 页爬取完毕！\n",
      "第 238 页爬取完毕！\n",
      "第 239 页爬取完毕！\n",
      "第 240 页爬取完毕！\n",
      "第 241 页爬取完毕！\n",
      "第 242 页爬取完毕！\n",
      "第 243 页爬取完毕！\n",
      "第 244 页爬取完毕！\n",
      "第 245 页爬取完毕！\n",
      "第 246 页爬取完毕！\n",
      "第 247 页爬取完毕！\n",
      "第 248 页爬取完毕！\n",
      "第 249 页爬取完毕！\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from lxml import etree\n",
    "import pandas as pd\n",
    "import time,random\n",
    "import json\n",
    "\n",
    "\n",
    "#url = 'https://search.51job.com/list/000000,000000,0000,00,9,99,%25E4%25BA%25BA%25E5%25B7%25A5%25E6%2599%25BA%25E8%2583%25BD,2,1.html?lang=c&postchannel=0000&workyear=99&cotype=99&degreefrom=99&jobterm=99&companysize=99&ord_field=0&dibiaoid=0&line=&welfare='\n",
    "url_qian = 'https://search.51job.com/list/000000,000000,0000,00,9,99,%25E4%25BA%25BA%25E5%25B7%25A5%25E6%2599%25BA%25E8%2583%25BD,2,'\n",
    "url_hou = '.html?lang=c&postchannel=0000&workyear=99&cotype=99&degreefrom=99&jobterm=99&companysize=99&ord_field=0&dibiaoid=0&line=&welfare='\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/65.0.3325.181 Safari/537.36'\n",
    "}\n",
    "\n",
    "for j in range(1,250):\n",
    "    url = url_qian+str(j)+url_hou\n",
    "    response = requests.get(url, headers=headers) # 发送网络请求\n",
    "    if response.status_code != 200:\n",
    "        print('第',j,'页爬取失败，进入下一页爬虫！')\n",
    "        continue\n",
    "    response.encoding = 'GBK'  # 设置中文方式编码\n",
    "    #print('文本源码', response.text)\n",
    "    #print('状态码', response.status_code)\n",
    "    \n",
    "    bs = BeautifulSoup(response.text,'html.parser')\n",
    "\n",
    "    #bs = bs.find_all('script',{'type':'text/javascript'})[2]\n",
    "    #info = re.findall('{\"type\":\"engine_search_result\",(.*?),\"adid\":\"\"}',str(bs))\n",
    "    \n",
    "    info = re.compile('window.__SEARCH_RESULT__ =(.+)</script>').findall(response.text)\n",
    "        \n",
    "    job_name = []  # 记录岗位名\n",
    "    company_name = []  # 记录公司名  \n",
    "    workarea_text = []  # 记录工作地点\n",
    "    salary = []  #记录工资\n",
    "    issuedate = []  # 记录发布时间\n",
    "    company_type = []  # 记录公司类型\n",
    "    company_size = []  # 记录公司人数\n",
    "    company_ind = []  # 记录行业\n",
    "    job_msg = []  # 工作描述\n",
    "\n",
    "    education = []#学历\n",
    "    work_experience = []#工作经验\n",
    "    limit_people = []#招聘人数\n",
    "    \n",
    "    for i in range(50):\n",
    "        #infoi=eval(\"{\"+info[i]+\"}\")\n",
    "        \n",
    "        infoi= json.loads(''.join(info))\n",
    "        infoi = infoi['engine_search_result'][i]\n",
    "        \n",
    "        job_name.append(re.sub(r'\\\\','',infoi['job_name']))\n",
    "        company_name.append(infoi['company_name'])\n",
    "        workarea_text.append(infoi['workarea_text'])\n",
    "        \n",
    "        salary.append(re.sub(r'\\\\','',infoi['providesalary_text']))\n",
    "        issuedate.append(infoi['issuedate'])\n",
    "        company_type.append(infoi['companytype_text'])\n",
    "        company_size.append(infoi['companysize_text'])\n",
    "        #company_ind.append(infoi['companyind_text'])\n",
    "        company_ind.append(re.sub(r'\\\\','',infoi['companyind_text']))\n",
    "\n",
    "        \n",
    "        education.append(\"\".join([i for i in infoi['attribute_text'] if i in '本科大专应届生在校生硕士博士']))# 通过列表推导式获取学历\n",
    "        work_experience.append(\"\".join([i for i in infoi['attribute_text'] if '经验' in i ]))\n",
    "        limit_people.append(\"\".join([i for i in infoi['attribute_text'] if '招' in i]))\n",
    "        \n",
    "        url_sub=infoi['job_href']\n",
    "        url_sub=re.sub('////','',re.sub(r'\\\\','//',url_sub))\n",
    "        response_sub=requests.get(url_sub,headers=headers)\n",
    "        response_sub.encoding = 'GBK'\n",
    "        dom = etree.HTML(response_sub.text)\n",
    "        job_msg.append(dom.xpath('//div[@class=\"tCompany_main\"]//div[@class=\"bmsg job_msg inbox\"]/p/text()'))   \n",
    "        #time.sleep(random.uniform(0,0.5))\n",
    "        \n",
    "    da = pd.DataFrame()\n",
    " \n",
    "    da['岗位名'] = job_name\n",
    "    da['公司名'] = company_name\n",
    "    da['工作地点'] = workarea_text\n",
    "    da['工资'] = salary\n",
    "    da['学历'] = education\n",
    "    da['工作经验'] = work_experience\n",
    "    da['招聘人数'] = limit_people\n",
    "    da['发布时间'] = issuedate\n",
    "    da['公司类型'] = company_type\n",
    "    da['公司人数'] = company_size\n",
    "    da['行业'] = company_ind\n",
    "    da['工作描述'] = job_msg \n",
    "    \n",
    "    \n",
    "    try:\n",
    "        da.to_csv('job_info_org1.csv', encoding='GBK', header=None)\n",
    "        print('第',j,'页爬取完毕！')\n",
    "        #print(da)\n",
    "        #time.sleep(random.uniform(0,0.5))\n",
    "    except:\n",
    "        print('第',j,'页爬取失败，进入下一页爬虫！')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
